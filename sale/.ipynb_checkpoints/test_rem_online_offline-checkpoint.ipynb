{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from utils.epsilon_decay import linearly_decaying_epsilon\n",
    "from models.box2d_models import DQNNetwork, MultiHeadQNetwork\n",
    "from replay_buffers.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "from multi_head_dqn import MultiHeadDQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online REM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save buffer every 100 episodes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/Prophet/sluo/software/anaconda3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "agent = MultiHeadDQNAgent(name='LunarLander-v2',\n",
    "                 network=MultiHeadQNetwork,\n",
    "                 num_actions=4,\n",
    "                 hiddens=[64,64], \n",
    "                 activation='relu',\n",
    "                 num_heads=5,\n",
    "                 num_convex_combinations=100,\n",
    "                 double=True,\n",
    "                 gamma=0.99,\n",
    "                 # optimizers\n",
    "                 optimizer=tf.keras.optimizers.Adam(\n",
    "                     tf.keras.optimizers.schedules.InverseTimeDecay(5e-4, decay_steps=100000, decay_rate=1)\n",
    "                 ),\n",
    "                 # replay buffer\n",
    "                 buffer_size=1000000,\n",
    "                 min_replay_history=1000,                 \n",
    "                 prioritized_replay=False,\n",
    "                 prioritized_replay_alpha=0.6,\n",
    "                 prioritized_replay_beta=0.4,\n",
    "                 online=True,\n",
    "                 persistent_directory='./trajs/multi_head_dqn/test/',\n",
    "                 episode_counts_to_save=100,\n",
    "                 sample_steps_to_refresh=10000,\n",
    "                 # training params\n",
    "                 max_training_steps=200000,\n",
    "                 training_steps_to_eval=1000,\n",
    "                 batch_size=64,\n",
    "                 max_episode_steps=1000,\n",
    "                 reward_clip=200,\n",
    "                 grad_clip=40,\n",
    "                 # stopping criteria\n",
    "                 target_mean_episode_reward=500,\n",
    "                 # target model update params\n",
    "                 tau=0.999,\n",
    "                 update_period=1,\n",
    "                 target_update_period=1,\n",
    "                 # exploration params\n",
    "                 epsilon_fn=linearly_decaying_epsilon,   \n",
    "                 epsilon_start=0.1,\n",
    "                 epsilon_decay_period=200000,\n",
    "                 epsilon_end=0.1,\n",
    "                 eval_mode=False,\n",
    "                 epsilon_eval=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "episodes 11\n",
      "timestep 1000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000495\n",
      "mean reward (100 episodes) -1232.616104\n",
      "max reward (100 episodes) -240.500045\n",
      "mean step (100 episodes) 164.600000\n",
      "max step (100 episodes) 272.000000\n",
      "------------------------------------------------\n",
      "episodes 26\n",
      "timestep 2000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000490\n",
      "mean reward (100 episodes) -691.820608\n",
      "max reward (100 episodes) -123.150411\n",
      "mean step (100 episodes) 120.000000\n",
      "max step (100 episodes) 272.000000\n",
      "------------------------------------------------\n",
      "episodes 37\n",
      "timestep 3000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000485\n",
      "mean reward (100 episodes) -666.565548\n",
      "max reward (100 episodes) -123.150411\n",
      "mean step (100 episodes) 272.200000\n",
      "max step (100 episodes) 747.000000\n",
      "------------------------------------------------\n",
      "episodes 39\n",
      "timestep 4000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000481\n",
      "mean reward (100 episodes) -536.344680\n",
      "max reward (100 episodes) -52.943780\n",
      "mean step (100 episodes) 299.050000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 41\n",
      "timestep 5000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000476\n",
      "mean reward (100 episodes) -472.154216\n",
      "max reward (100 episodes) -52.943780\n",
      "mean step (100 episodes) 321.600000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 44\n",
      "timestep 6000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000472\n",
      "mean reward (100 episodes) -448.679512\n",
      "max reward (100 episodes) -52.943780\n",
      "mean step (100 episodes) 359.700000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 47\n",
      "timestep 7000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000467\n",
      "mean reward (100 episodes) -409.163107\n",
      "max reward (100 episodes) -52.943780\n",
      "mean step (100 episodes) 436.971429\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 49\n",
      "timestep 8000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000463\n",
      "mean reward (100 episodes) -378.874694\n",
      "max reward (100 episodes) -52.943780\n",
      "mean step (100 episodes) 507.350000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 51\n",
      "timestep 9000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000459\n",
      "mean reward (100 episodes) -351.519244\n",
      "max reward (100 episodes) -52.943780\n",
      "mean step (100 episodes) 562.088889\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 53\n",
      "timestep 10000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000455\n",
      "mean reward (100 episodes) -328.796897\n",
      "max reward (100 episodes) -41.021679\n",
      "mean step (100 episodes) 594.420000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 55\n",
      "timestep 11000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000450\n",
      "mean reward (100 episodes) -312.351096\n",
      "max reward (100 episodes) -41.021679\n",
      "mean step (100 episodes) 631.290909\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 57\n",
      "timestep 12000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000446\n",
      "mean reward (100 episodes) -299.977459\n",
      "max reward (100 episodes) -41.021679\n",
      "mean step (100 episodes) 658.550000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 59\n",
      "timestep 13000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000442\n",
      "mean reward (100 episodes) -290.447653\n",
      "max reward (100 episodes) -41.021679\n",
      "mean step (100 episodes) 669.876923\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 61\n",
      "timestep 14000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000439\n",
      "mean reward (100 episodes) -284.931348\n",
      "max reward (100 episodes) -41.021679\n",
      "mean step (100 episodes) 671.628571\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 63\n",
      "timestep 15000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000435\n",
      "mean reward (100 episodes) -276.371740\n",
      "max reward (100 episodes) -41.021679\n",
      "mean step (100 episodes) 675.706667\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 65\n",
      "timestep 16000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000431\n",
      "mean reward (100 episodes) -268.898877\n",
      "max reward (100 episodes) -41.021679\n",
      "mean step (100 episodes) 692.037500\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 67\n",
      "timestep 17000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000427\n",
      "mean reward (100 episodes) -259.281747\n",
      "max reward (100 episodes) -20.701284\n",
      "mean step (100 episodes) 690.576471\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 70\n",
      "timestep 18000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000424\n",
      "mean reward (100 episodes) -248.721674\n",
      "max reward (100 episodes) 133.782280\n",
      "mean step (100 episodes) 697.455556\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 72\n",
      "timestep 19000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000420\n",
      "mean reward (100 episodes) -242.298402\n",
      "max reward (100 episodes) 133.782280\n",
      "mean step (100 episodes) 699.905263\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 75\n",
      "timestep 20000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000417\n",
      "mean reward (100 episodes) -239.862920\n",
      "max reward (100 episodes) 133.782280\n",
      "mean step (100 episodes) 706.000000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 79\n",
      "timestep 21000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000413\n",
      "mean reward (100 episodes) -184.372862\n",
      "max reward (100 episodes) 133.782280\n",
      "mean step (100 episodes) 730.160000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 81\n",
      "timestep 22000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000410\n",
      "mean reward (100 episodes) -183.529662\n",
      "max reward (100 episodes) 133.782280\n",
      "mean step (100 episodes) 756.640000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 84\n",
      "timestep 23000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000407\n",
      "mean reward (100 episodes) -157.954588\n",
      "max reward (100 episodes) 133.782280\n",
      "mean step (100 episodes) 760.840000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 86\n",
      "timestep 24000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000403\n",
      "mean reward (100 episodes) -156.523345\n",
      "max reward (100 episodes) 133.782280\n",
      "mean step (100 episodes) 776.770000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 89\n",
      "timestep 25000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000400\n",
      "mean reward (100 episodes) -151.592896\n",
      "max reward (100 episodes) 133.782280\n",
      "mean step (100 episodes) 792.060000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 91\n",
      "timestep 26000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000397\n",
      "mean reward (100 episodes) -140.583013\n",
      "max reward (100 episodes) 133.782280\n",
      "mean step (100 episodes) 798.940000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 93\n",
      "timestep 27000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000394\n",
      "mean reward (100 episodes) -135.008993\n",
      "max reward (100 episodes) 173.239610\n",
      "mean step (100 episodes) 791.140000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 95\n",
      "timestep 28000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000391\n",
      "mean reward (100 episodes) -131.563709\n",
      "max reward (100 episodes) 173.239610\n",
      "mean step (100 episodes) 791.140000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 98\n",
      "timestep 29000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000388\n",
      "mean reward (100 episodes) -126.811141\n",
      "max reward (100 episodes) 173.239610\n",
      "mean step (100 episodes) 758.980000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 100\n",
      "timestep 30000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000385\n",
      "mean reward (100 episodes) -119.575272\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 748.230000\n",
      "max step (100 episodes) 1000.000000\n",
      "Saved trajectories to save path: ./trajs/multi_head_dqn/test/trajs_0.pkl!\n",
      "------------------------------------------------\n",
      "episodes 102\n",
      "timestep 31000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000382\n",
      "mean reward (100 episodes) -115.251424\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 740.290000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "episodes 104\n",
      "timestep 32000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000379\n",
      "mean reward (100 episodes) -112.946332\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 719.930000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 106\n",
      "timestep 33000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000376\n",
      "mean reward (100 episodes) -104.357434\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 723.340000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 108\n",
      "timestep 34000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000373\n",
      "mean reward (100 episodes) -90.438301\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 730.780000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 110\n",
      "timestep 35000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000370\n",
      "mean reward (100 episodes) -85.188464\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 732.410000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 112\n",
      "timestep 36000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000368\n",
      "mean reward (100 episodes) -81.050417\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 715.920000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 114\n",
      "timestep 37000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000365\n",
      "mean reward (100 episodes) -77.450924\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 710.710000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 116\n",
      "timestep 38000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000362\n",
      "mean reward (100 episodes) -76.232315\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 712.180000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 118\n",
      "timestep 39000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000360\n",
      "mean reward (100 episodes) -71.376559\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 701.960000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 121\n",
      "timestep 40000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000357\n",
      "mean reward (100 episodes) -61.604578\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 702.750000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 123\n",
      "timestep 41000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000355\n",
      "mean reward (100 episodes) -56.056633\n",
      "max reward (100 episodes) 211.704709\n",
      "mean step (100 episodes) 717.320000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 126\n",
      "timestep 42000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000352\n",
      "mean reward (100 episodes) -48.344689\n",
      "max reward (100 episodes) 240.160954\n",
      "mean step (100 episodes) 716.470000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 129\n",
      "timestep 43000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000350\n",
      "mean reward (100 episodes) -50.058923\n",
      "max reward (100 episodes) 240.160954\n",
      "mean step (100 episodes) 716.790000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 132\n",
      "timestep 44000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000347\n",
      "mean reward (100 episodes) -43.939017\n",
      "max reward (100 episodes) 250.373660\n",
      "mean step (100 episodes) 710.910000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 134\n",
      "timestep 45000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000345\n",
      "mean reward (100 episodes) -41.038797\n",
      "max reward (100 episodes) 250.373660\n",
      "mean step (100 episodes) 702.460000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 136\n",
      "timestep 46000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000342\n",
      "mean reward (100 episodes) -33.805297\n",
      "max reward (100 episodes) 250.373660\n",
      "mean step (100 episodes) 700.080000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 139\n",
      "timestep 47000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000340\n",
      "mean reward (100 episodes) -34.759023\n",
      "max reward (100 episodes) 250.373660\n",
      "mean step (100 episodes) 697.400000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 141\n",
      "timestep 48000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000338\n",
      "mean reward (100 episodes) -33.097925\n",
      "max reward (100 episodes) 250.373660\n",
      "mean step (100 episodes) 690.930000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 143\n",
      "timestep 49000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000336\n",
      "mean reward (100 episodes) -33.763343\n",
      "max reward (100 episodes) 250.373660\n",
      "mean step (100 episodes) 718.690000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 145\n",
      "timestep 50000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000333\n",
      "mean reward (100 episodes) -27.584373\n",
      "max reward (100 episodes) 270.001943\n",
      "mean step (100 episodes) 708.160000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 147\n",
      "timestep 51000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000331\n",
      "mean reward (100 episodes) -25.193125\n",
      "max reward (100 episodes) 270.001943\n",
      "mean step (100 episodes) 706.750000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 149\n",
      "timestep 52000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000329\n",
      "mean reward (100 episodes) -24.768186\n",
      "max reward (100 episodes) 270.001943\n",
      "mean step (100 episodes) 714.560000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 151\n",
      "timestep 53000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000327\n",
      "mean reward (100 episodes) -26.659499\n",
      "max reward (100 episodes) 270.001943\n",
      "mean step (100 episodes) 701.660000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 155\n",
      "timestep 54000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000325\n",
      "mean reward (100 episodes) -31.372354\n",
      "max reward (100 episodes) 270.001943\n",
      "mean step (100 episodes) 706.160000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 158\n",
      "timestep 55000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000323\n",
      "mean reward (100 episodes) -27.014542\n",
      "max reward (100 episodes) 270.001943\n",
      "mean step (100 episodes) 699.390000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 160\n",
      "timestep 56000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000321\n",
      "mean reward (100 episodes) -21.893449\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 704.370000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 162\n",
      "timestep 57000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000318\n",
      "mean reward (100 episodes) -21.311294\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 710.800000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 164\n",
      "timestep 58000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000316\n",
      "mean reward (100 episodes) -18.256458\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 691.770000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 166\n",
      "timestep 59000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000314\n",
      "mean reward (100 episodes) -17.567396\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 714.790000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 168\n",
      "timestep 60000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000313\n",
      "mean reward (100 episodes) -17.813535\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 703.060000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 170\n",
      "timestep 61000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000311\n",
      "mean reward (100 episodes) -17.598334\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 689.900000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 173\n",
      "timestep 62000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000309\n",
      "mean reward (100 episodes) -19.285599\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 694.710000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "episodes 175\n",
      "timestep 63000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000307\n",
      "mean reward (100 episodes) -14.639735\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 711.360000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 177\n",
      "timestep 64000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000305\n",
      "mean reward (100 episodes) -14.203390\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 725.620000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 179\n",
      "timestep 65000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000303\n",
      "mean reward (100 episodes) -15.546680\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 745.680000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 181\n",
      "timestep 66000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000301\n",
      "mean reward (100 episodes) -24.874921\n",
      "max reward (100 episodes) 271.225236\n",
      "mean step (100 episodes) 733.340000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 185\n",
      "timestep 67000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000299\n",
      "mean reward (100 episodes) -20.469624\n",
      "max reward (100 episodes) 276.308589\n",
      "mean step (100 episodes) 731.870000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 187\n",
      "timestep 68000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000298\n",
      "mean reward (100 episodes) -17.380555\n",
      "max reward (100 episodes) 276.308589\n",
      "mean step (100 episodes) 730.030000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 189\n",
      "timestep 69000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000296\n",
      "mean reward (100 episodes) -14.481728\n",
      "max reward (100 episodes) 276.308589\n",
      "mean step (100 episodes) 731.930000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 193\n",
      "timestep 70000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000294\n",
      "mean reward (100 episodes) -20.718542\n",
      "max reward (100 episodes) 276.308589\n",
      "mean step (100 episodes) 755.500000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 195\n",
      "timestep 71000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000292\n",
      "mean reward (100 episodes) -27.882175\n",
      "max reward (100 episodes) 276.308589\n",
      "mean step (100 episodes) 742.470000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "episodes 197\n",
      "timestep 72000\n",
      "exploration 0.100000\n",
      "learning_rate 0.000291\n",
      "mean reward (100 episodes) -22.563679\n",
      "max reward (100 episodes) 276.308589\n",
      "mean step (100 episodes) 757.100000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    }
   ],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = pd.Series(agent.eval_episode_rewards)\n",
    "steps = pd.Series(agent.eval_episode_steps)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 8))\n",
    "\n",
    "axes[0][0].plot(rewards.rolling(100, min_periods=20).mean())\n",
    "axes[0][0].set_title('mean reward')\n",
    "axes[0][1].plot(rewards.rolling(100, min_periods=20).max())\n",
    "axes[0][1].set_title('max reward')\n",
    "axes[1][0].plot(steps.rolling(100, min_periods=20).mean())\n",
    "axes[1][0].set_title('mean step')\n",
    "axes[1][1].plot(steps.rolling(100, min_periods=20).max())\n",
    "axes[1][1].set_title('max step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._eval(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline REM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(5e-5, decay_steps=100000, decay_rate=1)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "agent = MultiHeadDQNAgent(name='LunarLander-v2',\n",
    "                         num_actions=4,\n",
    "                         hiddens=[64,64], \n",
    "                         activation='relu',\n",
    "                         num_heads=100,\n",
    "                         num_convex_combinations=100,\n",
    "                         gamma=0.99,                 \n",
    "                         # optimizers\n",
    "                         optimizer=optimizer,\n",
    "                         # replay buffer\n",
    "                         buffer_size=100000,\n",
    "                         min_replay_history=1000,                 \n",
    "                         prioritized_replay=False,\n",
    "                         prioritized_replay_alpha=0.6,\n",
    "                         prioritized_replay_beta=0.4,\n",
    "                         online=False,\n",
    "                         persistent_directory='./trajs/multi_head_dqn',\n",
    "                         episode_counts_to_save=100,\n",
    "                         sample_steps_to_refresh=500,\n",
    "                         # training params\n",
    "                         max_training_steps=500000,\n",
    "                         training_steps_to_eval=1000,\n",
    "                         batch_size=64,\n",
    "                         max_episode_steps=1000,\n",
    "                         # target model update params\n",
    "                         tau=0.999,\n",
    "                         update_period=1,\n",
    "                         target_update_period=1,\n",
    "                         # exploration params\n",
    "                         epsilon_fn=linearly_decaying_epsilon,        \n",
    "                         epsilon_start=0.1,\n",
    "                         epsilon_decay_period=100000,\n",
    "                         epsilon_end=0.1,\n",
    "                         eval_mode=False,\n",
    "                         epsilon_eval=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class WeightNetwork(tf.keras.Model):\n",
    "    def __init__(self, num_actions, \n",
    "                 hiddens=[16, 16], \n",
    "                 activation='relu', \n",
    "                 name='weight'):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.hiddens = hiddens\n",
    "        self.activation = activation\n",
    "        # defining layers\n",
    "        self.dense_layers = [tf.keras.layers.Dense(units=hidden, activation=activation)\n",
    "                             for hidden in hiddens]\n",
    "        self.out = tf.keras.layers.Dense(units=1, activation=None)\n",
    "        \n",
    "    def call(self, states, actions, future_states):\n",
    "        one_hot_actions = tf.one_hot(actions, depth=self.num_actions, \n",
    "                                     on_value=1.0, off_value=0.0)\n",
    "        x = tf.concat([states, one_hot_actions, future_states], axis=-1)\n",
    "        for dense in self.dense_layers:\n",
    "            x = dense(x)\n",
    "        return self.out(x)\n",
    "\n",
    "class VisitationRatioModel:\n",
    "    def __init__(self, model, optimizer, replay_buffer,\n",
    "                 target_policy, behavior_policy, medians=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.target_policy = target_policy\n",
    "        self.behavior_policy = behavior_policy\n",
    "        self.medians = medians\n",
    "        self.losses = []\n",
    "\n",
    "    def _compute_medians(self, n=100):\n",
    "        transitions = self.replay_buffer.sample(n)\n",
    "        states, actions, next_states = transitions[0], transitions[1], transitions[3]\n",
    "        mat = tf.concat([states, actions[:,np.newaxis], next_states], axis=-1)  # n x ...\n",
    "        dxx = tf.repeat(mat, n, axis=0) - tf.tile(mat, [n, 1])                  # n2 x ...\n",
    "        medians = np.median(tf.math.abs(dxx), axis=0) + 1e-2 # p\n",
    "        return medians\n",
    "    \n",
    "    def _normalize(self, weights, batch_size):\n",
    "        weights = tf.reshape(weights, [batch_size, batch_size])\n",
    "        weights_sum = tf.math.reduce_sum(weights, axis=1, keepdims=True) + 1e-6\n",
    "        weights = weights / weights_sum\n",
    "        return tf.reshape(weights, [batch_size**2])\n",
    "        \n",
    "    def _compute_loss(self, states, actions, next_states, gamma):\n",
    "        batch_size = states.shape[0]\n",
    "        \n",
    "        states_r = tf.repeat(states, batch_size, axis=0)      # n2 x ...\n",
    "        actions_r = tf.repeat(actions, batch_size)            # n2 \n",
    "        states_t = tf.tile(states, [batch_size, 1])           # n2 x ...\n",
    "        next_states_t = tf.tile(next_states, [batch_size, 1]) # n2 x ...\n",
    "        \n",
    "        ### state visitation ratios & policy ratios & deltas\n",
    "        weights = self.model(states_r, actions_r, states_t)           # n2\n",
    "        next_weights = self.model(states_r, actions_r, next_states_t) # n2\n",
    "        weights = self._normalize(weights, batch_size)                # n2\n",
    "        next_weights = self._normalize(next_weights, batch_size)      # n2 \n",
    "        policy_ratios = self.target_policy(states, actions) / (self.behavior_policy(states, actions)+1e-3) # n\n",
    "        policy_ratios = tf.tile(policy_ratios, [batch_size])          # n2\n",
    "        policy_ratios = tf.cast(policy_ratios, weights.dtype)\n",
    "        deltas = gamma * weights * policy_ratios - next_weights       # n2\n",
    "        \n",
    "        ### kernels\n",
    "        actions_r = tf.cast(actions_r, states.dtype)\n",
    "        mat1 = tf.concat([states, actions[:,None], next_states], axis=-1)        # n x ...\n",
    "        mat2 = tf.concat([states_r, actions_r[:,None], next_states_t], axis=-1)  # n2 x ...\n",
    "        dxx1 = tf.repeat(mat2, batch_size**2, axis=0) - tf.tile(mat2, [batch_size**2, 1]) # n4 x ...\n",
    "        dxx2 = tf.repeat(mat1, batch_size**2, axis=0) - tf.tile(mat2, [batch_size, 1])    # n3 x ...\n",
    "        dxx3 = tf.repeat(mat1, batch_size, axis=0)    - tf.tile(mat1, [batch_size, 1])    # n2 x ...\n",
    "        dxx1 = tf.exp(-tf.math.reduce_sum(tf.math.abs(dxx1)/self.medians, axis=-1)) # n4\n",
    "        dxx2 = tf.exp(-tf.math.reduce_sum(tf.math.abs(dxx2)/self.medians, axis=-1)) # n3\n",
    "        dxx3 = tf.exp(-tf.math.reduce_sum(tf.math.abs(dxx3)/self.medians, axis=-1)) # n2\n",
    "        \n",
    "        ### final loss\n",
    "        dxx1 = tf.repeat(deltas, batch_size**2) * tf.tile(deltas, [batch_size**2]) * dxx1\n",
    "        dxx2 = tf.tile(deltas, [batch_size]) * dxx2\n",
    "        loss = tf.reduce_sum(dxx1)/batch_size**4 + \\\n",
    "               2*(1-gamma)*tf.reduce_sum(dxx2)/batch_size**3 + \\\n",
    "               (1-gamma)**2*tf.reduce_sum(dxx3)/batch_size**2\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def fit(self, batch_size=32, gamma=0.99, max_iter=100):\n",
    "        if self.medians is None:\n",
    "            self.medians = self._compute_medians()\n",
    "            \n",
    "        for i in range(max_iter):\n",
    "            transitions = self.replay_buffer.sample(batch_size)\n",
    "            states, actions, next_states = transitions[0], transitions[1], transitions[3]\n",
    "            ##### compute loss function #####\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = self._compute_loss(states, actions, next_states, gamma)\n",
    "            dw = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(dw, self.model.trainable_variables))\n",
    "            \n",
    "            self.losses.append(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavior_policy(states, actions):\n",
    "    return 0.25*tf.ones(len(states))\n",
    "\n",
    "def target_policy(states, actions):\n",
    "    actions_ = np.argmax(agent.model(states).q_values, axis=1)\n",
    "    return (actions == actions_).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WeightNetwork(4)\n",
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(1e-3, decay_steps=100000, decay_rate=1)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "vrk = VisitationRatioModel(model, optimizer, agent.replay_buffer, target_policy, behavior_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = agent.replay_buffer.sample(32)\n",
    "states, actions, next_states = transitions[0], transitions[1], transitions[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrk.fit(batch_size=32, gamma=0.99, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [loss.numpy() for loss in vrk.losses]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
