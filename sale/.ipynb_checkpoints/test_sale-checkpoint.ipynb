{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from utils.epsilon_decay import linearly_decaying_epsilon\n",
    "from models.box2d_models import DQNNetwork\n",
    "from replay_buffers.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "from default_config import DEFAULT_CONFIG as config\n",
    "\n",
    "from dqn import DQNAgent\n",
    "from qr_dqn import QuantileAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SALE\n",
    "- cross validator\n",
    "- agent_1, ..., agent_K\n",
    "- Q values updator\n",
    "- advantage learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldCV:\n",
    "    def __init__(self, path, n_splits, shuffle=False, random_state=None):\n",
    "        self.path = path\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        with open(path, 'rb') as f:\n",
    "            self.trajs = pickle.load(f)\n",
    "        self.kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        \n",
    "    def split(self):\n",
    "        dirname = os.path.abspath(os.path.dirname(self.path))\n",
    "        \n",
    "        agent_paths = [dirname + '/agent_{}/'.format(k) for k in range(self.n_splits)]\n",
    "        for path in agent_paths:\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "                \n",
    "        train_paths = [path+'/train/' for path in agent_paths]\n",
    "        test_paths  = [path+'/test/'  for path in agent_paths]\n",
    "        for k, (train_index, test_index) in enumerate(self.kf.split(self.trajs)):\n",
    "            if not os.path.exists(train_paths[k]):\n",
    "                os.mkdir(train_paths[k]) \n",
    "            train_trajs = [self.trajs[index] for index in train_index]\n",
    "            with open(train_paths[k] + '/trajs_train_{}.pkl'.format(k), 'wb') as f:\n",
    "                pickle.dump(train_trajs, f)\n",
    "                \n",
    "            if not os.path.exists(test_paths[k]):\n",
    "                os.mkdir(test_paths[k])\n",
    "            test_trajs  = [self.trajs[index] for index in test_index]\n",
    "            with open(test_paths[k] + '/trajs_test_{}.pkl'.format(k), 'wb') as f:\n",
    "                pickle.dump(test_trajs, f)\n",
    "                \n",
    "        self.agent_paths = agent_paths\n",
    "        self.train_paths = train_paths\n",
    "        self.test_paths = test_paths\n",
    "        \n",
    "    # agents, behavior cloning, density ratios\n",
    "    def update_q(self, agents, bcs, density_ratios):\n",
    "        states_, actions_, qvalues_ = [], [], []\n",
    "        for k, (train_index, test_index) in enumerate(self.kf.split(self.trajs)):\n",
    "            test_trajs = [self.trajs[index] for index in test_index]\n",
    "            states = np.array([transition[0] for traj in test_trajs for transition in traj])\n",
    "            actions = np.array([transition[1] for traj in test_trajs for transition in traj])\n",
    "            rewards = np.array([transition[2] for traj in test_trajs for transition in traj])\n",
    "            next_states = np.array([transition[3] for traj in test_trajs for transition in traj])\n",
    "            \n",
    "            q_vals = agents[k].model(states).q_values\n",
    "            indices = tf.stack([tf.range(actions.shape[0]), actions], axis=-1)\n",
    "            chosen_q_vals = tf.gather_nd(q_vals, indices=indices)\n",
    "            next_vals = tf.math.reduce_max(agents[k].model(next_states).q_values, axis=1)\n",
    "            td_errors = rewards + agents[k].config['gamma'] * next_vals - q_vals\n",
    "            \n",
    "            psocres = bcs[k].policy(states, actions)\n",
    "            q_vals[range(len(actions)), actions] += (td_errors / (pscores + 1e-2)).clip(-100, 100)\n",
    "            \n",
    "            states_.append(states)\n",
    "            actions_.append(actions)\n",
    "            qvalues_.append(q_vals)\n",
    "        \n",
    "        states, actions, qvalues = np.vstack(states_), np.vstack(actions_), np.vstack(qvalues_)\n",
    "        \n",
    "        return [states, actions, qvalues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data1/Prophet/sluo/projects/SALE/result/dqn/trajs_dqn_pr.pkl'\n",
    "\n",
    "kf = KFoldCV(path, n_splits=2, shuffle=True, random_state=123456789)\n",
    "kf.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/data1/Prophet/sluo/projects/SALE/result/dqn/agent_0',\n",
       "  '/data1/Prophet/sluo/projects/SALE/result/dqn/agent_1'],\n",
       " ['/data1/Prophet/sluo/projects/SALE/result/dqn/agent_0/train/',\n",
       "  '/data1/Prophet/sluo/projects/SALE/result/dqn/agent_1/train/'],\n",
       " ['/data1/Prophet/sluo/projects/SALE/result/dqn/agent_0/test/',\n",
       "  '/data1/Prophet/sluo/projects/SALE/result/dqn/agent_1/test/'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf.agent_paths, kf.train_paths, kf.test_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test one case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['online'] = False\n",
    "config['max_training_steps'] = 500000\n",
    "config['lr'] = 1e-3\n",
    "config['decay_steps'] = 1000000\n",
    "\n",
    "config['persistent_directory'] = '/data1/Prophet/sluo/projects/SALE/result/dqn/agent/'\n",
    "config['checkpoint_path'] = '/data1/Prophet/sluo/projects/SALE/result/dqn/agent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/Prophet/sluo/software/anaconda3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trajectories from load path: /data1/Prophet/sluo/projects/SALE/result/dqn/agent/trajs_dqn_pr.pkl!\n",
      "Refresh buffer every 1000000 sampling!\n"
     ]
    }
   ],
   "source": [
    "agent = QuantileAgent(name='LunarLander-v2', num_actions=4, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "timestep 1000\n",
      "learning_rate 0.000999\n",
      "mean reward (100 episodes) -440.588287\n",
      "max reward (100 episodes) 3.351513\n",
      "mean step (100 episodes) 122.600000\n",
      "max step (100 episodes) 173.000000\n",
      "------------------------------------------------\n",
      "timestep 2000\n",
      "learning_rate 0.000998\n",
      "mean reward (100 episodes) -307.685373\n",
      "max reward (100 episodes) 222.296944\n",
      "mean step (100 episodes) 165.000000\n",
      "max step (100 episodes) 314.000000\n",
      "------------------------------------------------\n",
      "timestep 3000\n",
      "learning_rate 0.000997\n",
      "mean reward (100 episodes) -274.649304\n",
      "max reward (100 episodes) 222.296944\n",
      "mean step (100 episodes) 177.866667\n",
      "max step (100 episodes) 314.000000\n",
      "------------------------------------------------\n",
      "timestep 4000\n",
      "learning_rate 0.000996\n",
      "mean reward (100 episodes) -254.249604\n",
      "max reward (100 episodes) 222.296944\n",
      "mean step (100 episodes) 204.800000\n",
      "max step (100 episodes) 350.000000\n",
      "------------------------------------------------\n",
      "timestep 5000\n",
      "learning_rate 0.000995\n",
      "mean reward (100 episodes) -229.033138\n",
      "max reward (100 episodes) 222.296944\n",
      "mean step (100 episodes) 230.240000\n",
      "max step (100 episodes) 568.000000\n",
      "------------------------------------------------\n",
      "timestep 6000\n",
      "learning_rate 0.000994\n",
      "mean reward (100 episodes) -199.295845\n",
      "max reward (100 episodes) 222.296944\n",
      "mean step (100 episodes) 245.933333\n",
      "max step (100 episodes) 568.000000\n",
      "------------------------------------------------\n",
      "timestep 7000\n",
      "learning_rate 0.000993\n",
      "mean reward (100 episodes) -169.565636\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 264.885714\n",
      "max step (100 episodes) 568.000000\n",
      "------------------------------------------------\n",
      "timestep 8000\n",
      "learning_rate 0.000992\n",
      "mean reward (100 episodes) -140.075029\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 313.950000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 9000\n",
      "learning_rate 0.000991\n",
      "mean reward (100 episodes) -136.416035\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 390.177778\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_10000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 10000\n",
      "learning_rate 0.000990\n",
      "mean reward (100 episodes) -135.705783\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 451.160000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 11000\n",
      "learning_rate 0.000989\n",
      "mean reward (100 episodes) -132.911734\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 501.054545\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 12000\n",
      "learning_rate 0.000988\n",
      "mean reward (100 episodes) -129.315595\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 542.283333\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 13000\n",
      "learning_rate 0.000987\n",
      "mean reward (100 episodes) -118.031225\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 567.261538\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 14000\n",
      "learning_rate 0.000986\n",
      "mean reward (100 episodes) -109.623621\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 570.242857\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 15000\n",
      "learning_rate 0.000985\n",
      "mean reward (100 episodes) -106.000527\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 561.813333\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 16000\n",
      "learning_rate 0.000984\n",
      "mean reward (100 episodes) -96.588598\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 572.775000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 17000\n",
      "learning_rate 0.000983\n",
      "mean reward (100 episodes) -90.696190\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 572.458824\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 18000\n",
      "learning_rate 0.000982\n",
      "mean reward (100 episodes) -89.830777\n",
      "max reward (100 episodes) 232.775203\n",
      "mean step (100 episodes) 571.022222\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 19000\n",
      "learning_rate 0.000981\n",
      "mean reward (100 episodes) -85.113904\n",
      "max reward (100 episodes) 235.928915\n",
      "mean step (100 episodes) 561.842105\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_20000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 20000\n",
      "learning_rate 0.000980\n",
      "mean reward (100 episodes) -75.772591\n",
      "max reward (100 episodes) 235.928915\n",
      "mean step (100 episodes) 560.970000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 21000\n",
      "learning_rate 0.000979\n",
      "mean reward (100 episodes) -55.790262\n",
      "max reward (100 episodes) 235.928915\n",
      "mean step (100 episodes) 574.170000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 22000\n",
      "learning_rate 0.000978\n",
      "mean reward (100 episodes) -43.025118\n",
      "max reward (100 episodes) 235.928915\n",
      "mean step (100 episodes) 584.520000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 23000\n",
      "learning_rate 0.000978\n",
      "mean reward (100 episodes) -27.961275\n",
      "max reward (100 episodes) 235.928915\n",
      "mean step (100 episodes) 598.940000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 24000\n",
      "learning_rate 0.000977\n",
      "mean reward (100 episodes) -18.839849\n",
      "max reward (100 episodes) 235.928915\n",
      "mean step (100 episodes) 599.750000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 25000\n",
      "learning_rate 0.000976\n",
      "mean reward (100 episodes) -4.495791\n",
      "max reward (100 episodes) 240.196673\n",
      "mean step (100 episodes) 600.870000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 26000\n",
      "learning_rate 0.000975\n",
      "mean reward (100 episodes) 4.580021\n",
      "max reward (100 episodes) 240.196673\n",
      "mean step (100 episodes) 605.440000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 27000\n",
      "learning_rate 0.000974\n",
      "mean reward (100 episodes) 11.126701\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 614.430000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 28000\n",
      "learning_rate 0.000973\n",
      "mean reward (100 episodes) 12.710967\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 599.890000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 29000\n",
      "learning_rate 0.000972\n",
      "mean reward (100 episodes) 18.458042\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 562.850000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_30000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 30000\n",
      "learning_rate 0.000971\n",
      "mean reward (100 episodes) 31.555950\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 524.970000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 31000\n",
      "learning_rate 0.000970\n",
      "mean reward (100 episodes) 43.412201\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 485.990000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 32000\n",
      "learning_rate 0.000969\n",
      "mean reward (100 episodes) 57.742543\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 451.440000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 33000\n",
      "learning_rate 0.000968\n",
      "mean reward (100 episodes) 58.801011\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 419.610000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 34000\n",
      "learning_rate 0.000967\n",
      "mean reward (100 episodes) 68.118348\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 405.980000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "timestep 35000\n",
      "learning_rate 0.000966\n",
      "mean reward (100 episodes) 75.340695\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 394.280000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 36000\n",
      "learning_rate 0.000965\n",
      "mean reward (100 episodes) 76.606297\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 366.690000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 37000\n",
      "learning_rate 0.000964\n",
      "mean reward (100 episodes) 74.045190\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 342.700000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 38000\n",
      "learning_rate 0.000963\n",
      "mean reward (100 episodes) 80.720583\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 323.280000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 39000\n",
      "learning_rate 0.000962\n",
      "mean reward (100 episodes) 90.098657\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 325.200000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_40000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 40000\n",
      "learning_rate 0.000962\n",
      "mean reward (100 episodes) 81.649286\n",
      "max reward (100 episodes) 275.506534\n",
      "mean step (100 episodes) 302.990000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 41000\n",
      "learning_rate 0.000961\n",
      "mean reward (100 episodes) 93.940562\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 296.430000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 42000\n",
      "learning_rate 0.000960\n",
      "mean reward (100 episodes) 88.076365\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 280.930000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 43000\n",
      "learning_rate 0.000959\n",
      "mean reward (100 episodes) 87.441026\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 264.480000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 44000\n",
      "learning_rate 0.000958\n",
      "mean reward (100 episodes) 100.012015\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 263.860000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 45000\n",
      "learning_rate 0.000957\n",
      "mean reward (100 episodes) 93.820433\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 252.550000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 46000\n",
      "learning_rate 0.000956\n",
      "mean reward (100 episodes) 91.943732\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 239.780000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 47000\n",
      "learning_rate 0.000955\n",
      "mean reward (100 episodes) 85.070496\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 218.980000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 48000\n",
      "learning_rate 0.000954\n",
      "mean reward (100 episodes) 84.648844\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 209.670000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 49000\n",
      "learning_rate 0.000953\n",
      "mean reward (100 episodes) 81.163308\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 200.720000\n",
      "max step (100 episodes) 992.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_50000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 50000\n",
      "learning_rate 0.000952\n",
      "mean reward (100 episodes) 69.067547\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 192.590000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 51000\n",
      "learning_rate 0.000951\n",
      "mean reward (100 episodes) 62.524082\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 187.200000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 52000\n",
      "learning_rate 0.000951\n",
      "mean reward (100 episodes) 53.017076\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 177.950000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 53000\n",
      "learning_rate 0.000950\n",
      "mean reward (100 episodes) 50.407275\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 171.830000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 54000\n",
      "learning_rate 0.000949\n",
      "mean reward (100 episodes) 40.653732\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 160.260000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 55000\n",
      "learning_rate 0.000948\n",
      "mean reward (100 episodes) 33.700748\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 154.730000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 56000\n",
      "learning_rate 0.000947\n",
      "mean reward (100 episodes) 28.937957\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 149.710000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 57000\n",
      "learning_rate 0.000946\n",
      "mean reward (100 episodes) 30.212177\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 150.780000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 58000\n",
      "learning_rate 0.000945\n",
      "mean reward (100 episodes) 21.900572\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 146.740000\n",
      "max step (100 episodes) 992.000000\n",
      "------------------------------------------------\n",
      "timestep 59000\n",
      "learning_rate 0.000944\n",
      "mean reward (100 episodes) 7.115722\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 129.450000\n",
      "max step (100 episodes) 345.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_60000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 60000\n",
      "learning_rate 0.000943\n",
      "mean reward (100 episodes) 4.619409\n",
      "max reward (100 episodes) 285.543630\n",
      "mean step (100 episodes) 128.100000\n",
      "max step (100 episodes) 345.000000\n",
      "------------------------------------------------\n",
      "timestep 61000\n",
      "learning_rate 0.000943\n",
      "mean reward (100 episodes) -11.421114\n",
      "max reward (100 episodes) 279.216468\n",
      "mean step (100 episodes) 119.630000\n",
      "max step (100 episodes) 308.000000\n",
      "------------------------------------------------\n",
      "timestep 62000\n",
      "learning_rate 0.000942\n",
      "mean reward (100 episodes) -14.788274\n",
      "max reward (100 episodes) 279.216468\n",
      "mean step (100 episodes) 119.270000\n",
      "max step (100 episodes) 308.000000\n",
      "------------------------------------------------\n",
      "timestep 63000\n",
      "learning_rate 0.000941\n",
      "mean reward (100 episodes) -24.491507\n",
      "max reward (100 episodes) 279.216468\n",
      "mean step (100 episodes) 115.880000\n",
      "max step (100 episodes) 308.000000\n",
      "------------------------------------------------\n",
      "timestep 64000\n",
      "learning_rate 0.000940\n",
      "mean reward (100 episodes) -41.821348\n",
      "max reward (100 episodes) 279.216468\n",
      "mean step (100 episodes) 105.050000\n",
      "max step (100 episodes) 251.000000\n",
      "------------------------------------------------\n",
      "timestep 65000\n",
      "learning_rate 0.000939\n",
      "mean reward (100 episodes) -50.347763\n",
      "max reward (100 episodes) 279.216468\n",
      "mean step (100 episodes) 101.420000\n",
      "max step (100 episodes) 251.000000\n",
      "------------------------------------------------\n",
      "timestep 66000\n",
      "learning_rate 0.000938\n",
      "mean reward (100 episodes) -61.224129\n",
      "max reward (100 episodes) 279.216468\n",
      "mean step (100 episodes) 97.330000\n",
      "max step (100 episodes) 229.000000\n",
      "------------------------------------------------\n",
      "timestep 67000\n",
      "learning_rate 0.000937\n",
      "mean reward (100 episodes) -69.691391\n",
      "max reward (100 episodes) 279.216468\n",
      "mean step (100 episodes) 93.850000\n",
      "max step (100 episodes) 229.000000\n",
      "------------------------------------------------\n",
      "timestep 68000\n",
      "learning_rate 0.000936\n",
      "mean reward (100 episodes) -80.713609\n",
      "max reward (100 episodes) 75.444684\n",
      "mean step (100 episodes) 88.850000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 69000\n",
      "learning_rate 0.000935\n",
      "mean reward (100 episodes) -81.831079\n",
      "max reward (100 episodes) 75.444684\n",
      "mean step (100 episodes) 88.850000\n",
      "max step (100 episodes) 191.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_70000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 70000\n",
      "learning_rate 0.000935\n",
      "mean reward (100 episodes) -81.535343\n",
      "max reward (100 episodes) 75.444684\n",
      "mean step (100 episodes) 88.550000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 71000\n",
      "learning_rate 0.000934\n",
      "mean reward (100 episodes) -86.765133\n",
      "max reward (100 episodes) 75.444684\n",
      "mean step (100 episodes) 86.850000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 72000\n",
      "learning_rate 0.000933\n",
      "mean reward (100 episodes) -91.795952\n",
      "max reward (100 episodes) 75.444684\n",
      "mean step (100 episodes) 85.250000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 73000\n",
      "learning_rate 0.000932\n",
      "mean reward (100 episodes) -96.440340\n",
      "max reward (100 episodes) 75.444684\n",
      "mean step (100 episodes) 84.000000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 74000\n",
      "learning_rate 0.000931\n",
      "mean reward (100 episodes) -101.849086\n",
      "max reward (100 episodes) 75.444684\n",
      "mean step (100 episodes) 82.630000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 75000\n",
      "learning_rate 0.000930\n",
      "mean reward (100 episodes) -103.708137\n",
      "max reward (100 episodes) 75.444684\n",
      "mean step (100 episodes) 81.570000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 76000\n",
      "learning_rate 0.000929\n",
      "mean reward (100 episodes) -105.767918\n",
      "max reward (100 episodes) 25.272993\n",
      "mean step (100 episodes) 81.500000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 77000\n",
      "learning_rate 0.000929\n",
      "mean reward (100 episodes) -106.981930\n",
      "max reward (100 episodes) 2.474296\n",
      "mean step (100 episodes) 80.330000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 78000\n",
      "learning_rate 0.000928\n",
      "mean reward (100 episodes) -102.750417\n",
      "max reward (100 episodes) 2.474296\n",
      "mean step (100 episodes) 80.060000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 79000\n",
      "learning_rate 0.000927\n",
      "mean reward (100 episodes) -98.685431\n",
      "max reward (100 episodes) 2.474296\n",
      "mean step (100 episodes) 80.860000\n",
      "max step (100 episodes) 191.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_80000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 80000\n",
      "learning_rate 0.000926\n",
      "mean reward (100 episodes) -94.003771\n",
      "max reward (100 episodes) 2.474296\n",
      "mean step (100 episodes) 81.610000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 81000\n",
      "learning_rate 0.000925\n",
      "mean reward (100 episodes) -89.251175\n",
      "max reward (100 episodes) 45.470908\n",
      "mean step (100 episodes) 81.560000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 82000\n",
      "learning_rate 0.000924\n",
      "mean reward (100 episodes) -84.822694\n",
      "max reward (100 episodes) 45.470908\n",
      "mean step (100 episodes) 80.860000\n",
      "max step (100 episodes) 191.000000\n",
      "------------------------------------------------\n",
      "timestep 83000\n",
      "learning_rate 0.000923\n",
      "mean reward (100 episodes) -79.164828\n",
      "max reward (100 episodes) 45.470908\n",
      "mean step (100 episodes) 81.110000\n",
      "max step (100 episodes) 147.000000\n",
      "------------------------------------------------\n",
      "timestep 84000\n",
      "learning_rate 0.000923\n",
      "mean reward (100 episodes) -73.835759\n",
      "max reward (100 episodes) 45.470908\n",
      "mean step (100 episodes) 81.880000\n",
      "max step (100 episodes) 147.000000\n",
      "------------------------------------------------\n",
      "timestep 85000\n",
      "learning_rate 0.000922\n",
      "mean reward (100 episodes) -68.610682\n",
      "max reward (100 episodes) 45.470908\n",
      "mean step (100 episodes) 83.250000\n",
      "max step (100 episodes) 147.000000\n",
      "------------------------------------------------\n",
      "timestep 86000\n",
      "learning_rate 0.000921\n",
      "mean reward (100 episodes) -62.650807\n",
      "max reward (100 episodes) 45.470908\n",
      "mean step (100 episodes) 83.540000\n",
      "max step (100 episodes) 147.000000\n",
      "------------------------------------------------\n",
      "timestep 87000\n",
      "learning_rate 0.000920\n",
      "mean reward (100 episodes) -56.039860\n",
      "max reward (100 episodes) 45.470908\n",
      "mean step (100 episodes) 84.630000\n",
      "max step (100 episodes) 147.000000\n",
      "------------------------------------------------\n",
      "timestep 88000\n",
      "learning_rate 0.000919\n",
      "mean reward (100 episodes) -46.465947\n",
      "max reward (100 episodes) 164.495734\n",
      "mean step (100 episodes) 95.150000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 89000\n",
      "learning_rate 0.000918\n",
      "mean reward (100 episodes) -40.452713\n",
      "max reward (100 episodes) 164.495734\n",
      "mean step (100 episodes) 96.320000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_90000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 90000\n",
      "learning_rate 0.000917\n",
      "mean reward (100 episodes) -29.819879\n",
      "max reward (100 episodes) 293.277195\n",
      "mean step (100 episodes) 100.960000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 91000\n",
      "learning_rate 0.000917\n",
      "mean reward (100 episodes) -16.386278\n",
      "max reward (100 episodes) 293.277195\n",
      "mean step (100 episodes) 121.850000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 92000\n",
      "learning_rate 0.000916\n",
      "mean reward (100 episodes) -7.468510\n",
      "max reward (100 episodes) 293.277195\n",
      "mean step (100 episodes) 123.770000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 93000\n",
      "learning_rate 0.000915\n",
      "mean reward (100 episodes) 4.190643\n",
      "max reward (100 episodes) 293.277195\n",
      "mean step (100 episodes) 126.250000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 94000\n",
      "learning_rate 0.000914\n",
      "mean reward (100 episodes) 21.888126\n",
      "max reward (100 episodes) 293.277195\n",
      "mean step (100 episodes) 143.090000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 95000\n",
      "learning_rate 0.000913\n",
      "mean reward (100 episodes) 30.157647\n",
      "max reward (100 episodes) 293.277195\n",
      "mean step (100 episodes) 149.340000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 96000\n",
      "learning_rate 0.000912\n",
      "mean reward (100 episodes) 46.418211\n",
      "max reward (100 episodes) 295.978732\n",
      "mean step (100 episodes) 154.410000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 97000\n",
      "learning_rate 0.000912\n",
      "mean reward (100 episodes) 56.075748\n",
      "max reward (100 episodes) 295.978732\n",
      "mean step (100 episodes) 158.540000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 98000\n",
      "learning_rate 0.000911\n",
      "mean reward (100 episodes) 64.953777\n",
      "max reward (100 episodes) 295.978732\n",
      "mean step (100 episodes) 179.850000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 99000\n",
      "learning_rate 0.000910\n",
      "mean reward (100 episodes) 73.914685\n",
      "max reward (100 episodes) 295.978732\n",
      "mean step (100 episodes) 187.330000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_100000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 100000\n",
      "learning_rate 0.000909\n",
      "mean reward (100 episodes) 80.674585\n",
      "max reward (100 episodes) 295.978732\n",
      "mean step (100 episodes) 214.640000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 101000\n",
      "learning_rate 0.000908\n",
      "mean reward (100 episodes) 94.955022\n",
      "max reward (100 episodes) 295.978732\n",
      "mean step (100 episodes) 229.020000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 102000\n",
      "learning_rate 0.000907\n",
      "mean reward (100 episodes) 101.250598\n",
      "max reward (100 episodes) 298.320503\n",
      "mean step (100 episodes) 237.910000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 103000\n",
      "learning_rate 0.000907\n",
      "mean reward (100 episodes) 106.577697\n",
      "max reward (100 episodes) 298.320503\n",
      "mean step (100 episodes) 248.710000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "timestep 104000\n",
      "learning_rate 0.000906\n",
      "mean reward (100 episodes) 115.908320\n",
      "max reward (100 episodes) 298.320503\n",
      "mean step (100 episodes) 264.770000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 105000\n",
      "learning_rate 0.000905\n",
      "mean reward (100 episodes) 124.155426\n",
      "max reward (100 episodes) 298.320503\n",
      "mean step (100 episodes) 288.050000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 106000\n",
      "learning_rate 0.000904\n",
      "mean reward (100 episodes) 129.664459\n",
      "max reward (100 episodes) 298.320503\n",
      "mean step (100 episodes) 293.570000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 107000\n",
      "learning_rate 0.000903\n",
      "mean reward (100 episodes) 137.626328\n",
      "max reward (100 episodes) 298.320503\n",
      "mean step (100 episodes) 308.020000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 108000\n",
      "learning_rate 0.000903\n",
      "mean reward (100 episodes) 132.123731\n",
      "max reward (100 episodes) 298.320503\n",
      "mean step (100 episodes) 314.990000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 109000\n",
      "learning_rate 0.000902\n",
      "mean reward (100 episodes) 139.727066\n",
      "max reward (100 episodes) 298.320503\n",
      "mean step (100 episodes) 334.360000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_110000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 110000\n",
      "learning_rate 0.000901\n",
      "mean reward (100 episodes) 131.077422\n",
      "max reward (100 episodes) 298.320503\n",
      "mean step (100 episodes) 330.710000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 111000\n",
      "learning_rate 0.000900\n",
      "mean reward (100 episodes) 128.317373\n",
      "max reward (100 episodes) 298.323567\n",
      "mean step (100 episodes) 325.570000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 112000\n",
      "learning_rate 0.000899\n",
      "mean reward (100 episodes) 132.920854\n",
      "max reward (100 episodes) 298.323567\n",
      "mean step (100 episodes) 345.460000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 113000\n",
      "learning_rate 0.000898\n",
      "mean reward (100 episodes) 134.324879\n",
      "max reward (100 episodes) 298.323567\n",
      "mean step (100 episodes) 366.530000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 114000\n",
      "learning_rate 0.000898\n",
      "mean reward (100 episodes) 124.298681\n",
      "max reward (100 episodes) 298.323567\n",
      "mean step (100 episodes) 370.470000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 115000\n",
      "learning_rate 0.000897\n",
      "mean reward (100 episodes) 114.933057\n",
      "max reward (100 episodes) 298.323567\n",
      "mean step (100 episodes) 372.990000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 116000\n",
      "learning_rate 0.000896\n",
      "mean reward (100 episodes) 109.220498\n",
      "max reward (100 episodes) 298.323567\n",
      "mean step (100 episodes) 406.050000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 117000\n",
      "learning_rate 0.000895\n",
      "mean reward (100 episodes) 106.531251\n",
      "max reward (100 episodes) 298.323567\n",
      "mean step (100 episodes) 439.660000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 118000\n",
      "learning_rate 0.000894\n",
      "mean reward (100 episodes) 98.777441\n",
      "max reward (100 episodes) 298.323567\n",
      "mean step (100 episodes) 457.030000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 119000\n",
      "learning_rate 0.000894\n",
      "mean reward (100 episodes) 98.254858\n",
      "max reward (100 episodes) 298.323567\n",
      "mean step (100 episodes) 472.230000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_120000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 120000\n",
      "learning_rate 0.000893\n",
      "mean reward (100 episodes) 104.307574\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 463.580000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 121000\n",
      "learning_rate 0.000892\n",
      "mean reward (100 episodes) 100.257399\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 466.110000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 122000\n",
      "learning_rate 0.000891\n",
      "mean reward (100 episodes) 99.377197\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 472.130000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 123000\n",
      "learning_rate 0.000890\n",
      "mean reward (100 episodes) 90.997843\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 479.530000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 124000\n",
      "learning_rate 0.000890\n",
      "mean reward (100 episodes) 84.574895\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 493.010000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 125000\n",
      "learning_rate 0.000889\n",
      "mean reward (100 episodes) 86.629314\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 496.190000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 126000\n",
      "learning_rate 0.000888\n",
      "mean reward (100 episodes) 82.242184\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 529.010000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 127000\n",
      "learning_rate 0.000887\n",
      "mean reward (100 episodes) 79.878996\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 549.650000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 128000\n",
      "learning_rate 0.000887\n",
      "mean reward (100 episodes) 93.472084\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 551.880000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 129000\n",
      "learning_rate 0.000886\n",
      "mean reward (100 episodes) 90.717496\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 553.000000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_130000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 130000\n",
      "learning_rate 0.000885\n",
      "mean reward (100 episodes) 99.023667\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 569.940000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 131000\n",
      "learning_rate 0.000884\n",
      "mean reward (100 episodes) 99.397104\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 586.380000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 132000\n",
      "learning_rate 0.000883\n",
      "mean reward (100 episodes) 91.965540\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 603.800000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 133000\n",
      "learning_rate 0.000883\n",
      "mean reward (100 episodes) 88.803295\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 606.970000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 134000\n",
      "learning_rate 0.000882\n",
      "mean reward (100 episodes) 90.456117\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 611.420000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 135000\n",
      "learning_rate 0.000881\n",
      "mean reward (100 episodes) 102.802501\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 635.450000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 136000\n",
      "learning_rate 0.000880\n",
      "mean reward (100 episodes) 95.541744\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 610.010000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 137000\n",
      "learning_rate 0.000880\n",
      "mean reward (100 episodes) 99.213475\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 590.590000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "timestep 138000\n",
      "learning_rate 0.000879\n",
      "mean reward (100 episodes) 110.105396\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 565.730000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 139000\n",
      "learning_rate 0.000878\n",
      "mean reward (100 episodes) 110.119609\n",
      "max reward (100 episodes) 298.904891\n",
      "mean step (100 episodes) 570.910000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_140000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 140000\n",
      "learning_rate 0.000877\n",
      "mean reward (100 episodes) 106.925118\n",
      "max reward (100 episodes) 286.325140\n",
      "mean step (100 episodes) 568.740000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 141000\n",
      "learning_rate 0.000876\n",
      "mean reward (100 episodes) 103.069714\n",
      "max reward (100 episodes) 286.325140\n",
      "mean step (100 episodes) 565.090000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 142000\n",
      "learning_rate 0.000876\n",
      "mean reward (100 episodes) 99.511289\n",
      "max reward (100 episodes) 286.325140\n",
      "mean step (100 episodes) 561.390000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 143000\n",
      "learning_rate 0.000875\n",
      "mean reward (100 episodes) 106.080658\n",
      "max reward (100 episodes) 286.325140\n",
      "mean step (100 episodes) 574.050000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 144000\n",
      "learning_rate 0.000874\n",
      "mean reward (100 episodes) 111.174317\n",
      "max reward (100 episodes) 282.152890\n",
      "mean step (100 episodes) 572.150000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 145000\n",
      "learning_rate 0.000873\n",
      "mean reward (100 episodes) 100.262320\n",
      "max reward (100 episodes) 282.152890\n",
      "mean step (100 episodes) 568.300000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 146000\n",
      "learning_rate 0.000873\n",
      "mean reward (100 episodes) 100.318922\n",
      "max reward (100 episodes) 282.152890\n",
      "mean step (100 episodes) 544.400000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 147000\n",
      "learning_rate 0.000872\n",
      "mean reward (100 episodes) 99.726756\n",
      "max reward (100 episodes) 282.152890\n",
      "mean step (100 episodes) 517.870000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 148000\n",
      "learning_rate 0.000871\n",
      "mean reward (100 episodes) 91.382862\n",
      "max reward (100 episodes) 282.152890\n",
      "mean step (100 episodes) 509.560000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 149000\n",
      "learning_rate 0.000870\n",
      "mean reward (100 episodes) 90.294214\n",
      "max reward (100 episodes) 254.149402\n",
      "mean step (100 episodes) 499.000000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_150000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 150000\n",
      "learning_rate 0.000870\n",
      "mean reward (100 episodes) 89.816046\n",
      "max reward (100 episodes) 254.149402\n",
      "mean step (100 episodes) 493.630000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 151000\n",
      "learning_rate 0.000869\n",
      "mean reward (100 episodes) 88.335630\n",
      "max reward (100 episodes) 254.149402\n",
      "mean step (100 episodes) 476.980000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 152000\n",
      "learning_rate 0.000868\n",
      "mean reward (100 episodes) 90.290337\n",
      "max reward (100 episodes) 254.149402\n",
      "mean step (100 episodes) 469.870000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 153000\n",
      "learning_rate 0.000867\n",
      "mean reward (100 episodes) 88.920593\n",
      "max reward (100 episodes) 254.149402\n",
      "mean step (100 episodes) 459.690000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 154000\n",
      "learning_rate 0.000867\n",
      "mean reward (100 episodes) 87.221254\n",
      "max reward (100 episodes) 254.149402\n",
      "mean step (100 episodes) 475.650000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 155000\n",
      "learning_rate 0.000866\n",
      "mean reward (100 episodes) 88.875526\n",
      "max reward (100 episodes) 254.149402\n",
      "mean step (100 episodes) 458.360000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 156000\n",
      "learning_rate 0.000865\n",
      "mean reward (100 episodes) 93.929492\n",
      "max reward (100 episodes) 254.149402\n",
      "mean step (100 episodes) 466.690000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 157000\n",
      "learning_rate 0.000864\n",
      "mean reward (100 episodes) 88.814747\n",
      "max reward (100 episodes) 253.446433\n",
      "mean step (100 episodes) 455.620000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 158000\n",
      "learning_rate 0.000864\n",
      "mean reward (100 episodes) 76.087094\n",
      "max reward (100 episodes) 253.446433\n",
      "mean step (100 episodes) 463.550000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 159000\n",
      "learning_rate 0.000863\n",
      "mean reward (100 episodes) 71.707628\n",
      "max reward (100 episodes) 253.446433\n",
      "mean step (100 episodes) 449.130000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_160000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 160000\n",
      "learning_rate 0.000862\n",
      "mean reward (100 episodes) 64.116977\n",
      "max reward (100 episodes) 253.446433\n",
      "mean step (100 episodes) 461.260000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 161000\n",
      "learning_rate 0.000861\n",
      "mean reward (100 episodes) 63.752849\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 480.990000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 162000\n",
      "learning_rate 0.000861\n",
      "mean reward (100 episodes) 71.865967\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 479.640000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 163000\n",
      "learning_rate 0.000860\n",
      "mean reward (100 episodes) 70.877389\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 460.210000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 164000\n",
      "learning_rate 0.000859\n",
      "mean reward (100 episodes) 65.347386\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 455.160000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 165000\n",
      "learning_rate 0.000858\n",
      "mean reward (100 episodes) 70.274990\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 453.120000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 166000\n",
      "learning_rate 0.000858\n",
      "mean reward (100 episodes) 67.963185\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 474.370000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 167000\n",
      "learning_rate 0.000857\n",
      "mean reward (100 episodes) 69.891389\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 490.370000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 168000\n",
      "learning_rate 0.000856\n",
      "mean reward (100 episodes) 68.810730\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 494.190000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 169000\n",
      "learning_rate 0.000855\n",
      "mean reward (100 episodes) 68.242382\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 511.460000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_170000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 170000\n",
      "learning_rate 0.000855\n",
      "mean reward (100 episodes) 64.426379\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 527.530000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 171000\n",
      "learning_rate 0.000854\n",
      "mean reward (100 episodes) 62.255254\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 531.740000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "timestep 172000\n",
      "learning_rate 0.000853\n",
      "mean reward (100 episodes) 54.019665\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 545.230000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 173000\n",
      "learning_rate 0.000853\n",
      "mean reward (100 episodes) 55.812389\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 545.980000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 174000\n",
      "learning_rate 0.000852\n",
      "mean reward (100 episodes) 56.063156\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 543.460000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 175000\n",
      "learning_rate 0.000851\n",
      "mean reward (100 episodes) 50.991514\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 556.280000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 176000\n",
      "learning_rate 0.000850\n",
      "mean reward (100 episodes) 50.708034\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 571.610000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 177000\n",
      "learning_rate 0.000850\n",
      "mean reward (100 episodes) 54.513034\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 591.220000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 178000\n",
      "learning_rate 0.000849\n",
      "mean reward (100 episodes) 62.138990\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 595.520000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 179000\n",
      "learning_rate 0.000848\n",
      "mean reward (100 episodes) 64.209517\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 617.360000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_180000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 180000\n",
      "learning_rate 0.000847\n",
      "mean reward (100 episodes) 63.326640\n",
      "max reward (100 episodes) 288.536793\n",
      "mean step (100 episodes) 625.460000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 181000\n",
      "learning_rate 0.000847\n",
      "mean reward (100 episodes) 57.186445\n",
      "max reward (100 episodes) 276.520628\n",
      "mean step (100 episodes) 633.070000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 182000\n",
      "learning_rate 0.000846\n",
      "mean reward (100 episodes) 55.570238\n",
      "max reward (100 episodes) 286.056830\n",
      "mean step (100 episodes) 655.840000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 183000\n",
      "learning_rate 0.000845\n",
      "mean reward (100 episodes) 59.615386\n",
      "max reward (100 episodes) 286.056830\n",
      "mean step (100 episodes) 669.850000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 184000\n",
      "learning_rate 0.000845\n",
      "mean reward (100 episodes) 69.158967\n",
      "max reward (100 episodes) 286.056830\n",
      "mean step (100 episodes) 667.100000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 185000\n",
      "learning_rate 0.000844\n",
      "mean reward (100 episodes) 70.684523\n",
      "max reward (100 episodes) 286.056830\n",
      "mean step (100 episodes) 674.740000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 186000\n",
      "learning_rate 0.000843\n",
      "mean reward (100 episodes) 83.415495\n",
      "max reward (100 episodes) 286.056830\n",
      "mean step (100 episodes) 655.900000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 187000\n",
      "learning_rate 0.000842\n",
      "mean reward (100 episodes) 84.343824\n",
      "max reward (100 episodes) 286.056830\n",
      "mean step (100 episodes) 666.630000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 188000\n",
      "learning_rate 0.000842\n",
      "mean reward (100 episodes) 91.981092\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 664.200000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 189000\n",
      "learning_rate 0.000841\n",
      "mean reward (100 episodes) 97.538868\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 651.380000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_190000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 190000\n",
      "learning_rate 0.000840\n",
      "mean reward (100 episodes) 104.431393\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 640.370000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 191000\n",
      "learning_rate 0.000840\n",
      "mean reward (100 episodes) 109.418470\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 649.680000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 192000\n",
      "learning_rate 0.000839\n",
      "mean reward (100 episodes) 124.264252\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 629.920000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 193000\n",
      "learning_rate 0.000838\n",
      "mean reward (100 episodes) 126.585614\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 624.920000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 194000\n",
      "learning_rate 0.000838\n",
      "mean reward (100 episodes) 134.536159\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 612.580000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 195000\n",
      "learning_rate 0.000837\n",
      "mean reward (100 episodes) 141.513040\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 609.820000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 196000\n",
      "learning_rate 0.000836\n",
      "mean reward (100 episodes) 145.332971\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 592.370000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 197000\n",
      "learning_rate 0.000835\n",
      "mean reward (100 episodes) 147.914339\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 578.800000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 198000\n",
      "learning_rate 0.000835\n",
      "mean reward (100 episodes) 151.817486\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 581.050000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 199000\n",
      "learning_rate 0.000834\n",
      "mean reward (100 episodes) 155.326773\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 568.140000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_200000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 200000\n",
      "learning_rate 0.000833\n",
      "mean reward (100 episodes) 166.208720\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 547.240000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 201000\n",
      "learning_rate 0.000833\n",
      "mean reward (100 episodes) 177.670528\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 533.920000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 202000\n",
      "learning_rate 0.000832\n",
      "mean reward (100 episodes) 180.172648\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 526.190000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 203000\n",
      "learning_rate 0.000831\n",
      "mean reward (100 episodes) 180.279197\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 516.090000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 204000\n",
      "learning_rate 0.000831\n",
      "mean reward (100 episodes) 179.792701\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 511.660000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 205000\n",
      "learning_rate 0.000830\n",
      "mean reward (100 episodes) 180.425313\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 519.000000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "timestep 206000\n",
      "learning_rate 0.000829\n",
      "mean reward (100 episodes) 176.920928\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 510.180000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 207000\n",
      "learning_rate 0.000829\n",
      "mean reward (100 episodes) 181.243080\n",
      "max reward (100 episodes) 316.000206\n",
      "mean step (100 episodes) 483.900000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 208000\n",
      "learning_rate 0.000828\n",
      "mean reward (100 episodes) 180.457500\n",
      "max reward (100 episodes) 301.838766\n",
      "mean step (100 episodes) 489.410000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 209000\n",
      "learning_rate 0.000827\n",
      "mean reward (100 episodes) 182.323483\n",
      "max reward (100 episodes) 301.838766\n",
      "mean step (100 episodes) 483.190000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_210000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 210000\n",
      "learning_rate 0.000826\n",
      "mean reward (100 episodes) 179.435892\n",
      "max reward (100 episodes) 301.838766\n",
      "mean step (100 episodes) 474.180000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 211000\n",
      "learning_rate 0.000826\n",
      "mean reward (100 episodes) 178.664940\n",
      "max reward (100 episodes) 301.838766\n",
      "mean step (100 episodes) 469.740000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 212000\n",
      "learning_rate 0.000825\n",
      "mean reward (100 episodes) 180.381318\n",
      "max reward (100 episodes) 301.838766\n",
      "mean step (100 episodes) 459.800000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 213000\n",
      "learning_rate 0.000824\n",
      "mean reward (100 episodes) 177.723899\n",
      "max reward (100 episodes) 301.838766\n",
      "mean step (100 episodes) 467.850000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 214000\n",
      "learning_rate 0.000824\n",
      "mean reward (100 episodes) 169.415873\n",
      "max reward (100 episodes) 301.838766\n",
      "mean step (100 episodes) 456.480000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 215000\n",
      "learning_rate 0.000823\n",
      "mean reward (100 episodes) 166.154020\n",
      "max reward (100 episodes) 301.838766\n",
      "mean step (100 episodes) 472.430000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 216000\n",
      "learning_rate 0.000822\n",
      "mean reward (100 episodes) 160.887940\n",
      "max reward (100 episodes) 281.946459\n",
      "mean step (100 episodes) 469.400000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 217000\n",
      "learning_rate 0.000822\n",
      "mean reward (100 episodes) 156.610780\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 472.920000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 218000\n",
      "learning_rate 0.000821\n",
      "mean reward (100 episodes) 159.083729\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 462.490000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 219000\n",
      "learning_rate 0.000820\n",
      "mean reward (100 episodes) 155.485660\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 445.010000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_220000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 220000\n",
      "learning_rate 0.000820\n",
      "mean reward (100 episodes) 151.131664\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 450.150000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 221000\n",
      "learning_rate 0.000819\n",
      "mean reward (100 episodes) 139.059484\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 431.440000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 222000\n",
      "learning_rate 0.000818\n",
      "mean reward (100 episodes) 137.577310\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 418.290000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 223000\n",
      "learning_rate 0.000818\n",
      "mean reward (100 episodes) 130.741909\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 432.480000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 224000\n",
      "learning_rate 0.000817\n",
      "mean reward (100 episodes) 122.639466\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 426.300000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 225000\n",
      "learning_rate 0.000816\n",
      "mean reward (100 episodes) 116.937368\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 414.410000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 226000\n",
      "learning_rate 0.000816\n",
      "mean reward (100 episodes) 110.837048\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 417.860000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 227000\n",
      "learning_rate 0.000815\n",
      "mean reward (100 episodes) 106.435351\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 420.270000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 228000\n",
      "learning_rate 0.000814\n",
      "mean reward (100 episodes) 101.525148\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 412.660000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 229000\n",
      "learning_rate 0.000814\n",
      "mean reward (100 episodes) 91.834513\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 418.480000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_230000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 230000\n",
      "learning_rate 0.000813\n",
      "mean reward (100 episodes) 91.603690\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 419.900000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 231000\n",
      "learning_rate 0.000812\n",
      "mean reward (100 episodes) 90.745007\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 403.560000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 232000\n",
      "learning_rate 0.000812\n",
      "mean reward (100 episodes) 75.283389\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 397.150000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 233000\n",
      "learning_rate 0.000811\n",
      "mean reward (100 episodes) 72.157572\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 382.220000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 234000\n",
      "learning_rate 0.000810\n",
      "mean reward (100 episodes) 73.087426\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 384.210000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 235000\n",
      "learning_rate 0.000810\n",
      "mean reward (100 episodes) 69.115179\n",
      "max reward (100 episodes) 304.759077\n",
      "mean step (100 episodes) 354.200000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 236000\n",
      "learning_rate 0.000809\n",
      "mean reward (100 episodes) 67.837205\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 349.100000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 237000\n",
      "learning_rate 0.000808\n",
      "mean reward (100 episodes) 64.265688\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 347.280000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 238000\n",
      "learning_rate 0.000808\n",
      "mean reward (100 episodes) 50.906440\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 335.150000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 239000\n",
      "learning_rate 0.000807\n",
      "mean reward (100 episodes) 46.217186\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 331.680000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_240000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 240000\n",
      "learning_rate 0.000806\n",
      "mean reward (100 episodes) 42.043888\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 321.000000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 241000\n",
      "learning_rate 0.000806\n",
      "mean reward (100 episodes) 40.519939\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 319.960000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 242000\n",
      "learning_rate 0.000805\n",
      "mean reward (100 episodes) 34.060829\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 317.370000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 243000\n",
      "learning_rate 0.000805\n",
      "mean reward (100 episodes) 35.087178\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 292.310000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 244000\n",
      "learning_rate 0.000804\n",
      "mean reward (100 episodes) 31.513406\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 292.170000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 245000\n",
      "learning_rate 0.000803\n",
      "mean reward (100 episodes) 33.680831\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 273.110000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 246000\n",
      "learning_rate 0.000803\n",
      "mean reward (100 episodes) 39.985744\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 268.820000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 247000\n",
      "learning_rate 0.000802\n",
      "mean reward (100 episodes) 31.547151\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 259.060000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 248000\n",
      "learning_rate 0.000801\n",
      "mean reward (100 episodes) 31.429374\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 257.350000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 249000\n",
      "learning_rate 0.000801\n",
      "mean reward (100 episodes) 30.098974\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 244.370000\n",
      "max step (100 episodes) 1000.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_250000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 250000\n",
      "learning_rate 0.000800\n",
      "mean reward (100 episodes) 22.278223\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 236.200000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 251000\n",
      "learning_rate 0.000799\n",
      "mean reward (100 episodes) 12.871700\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 229.720000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 252000\n",
      "learning_rate 0.000799\n",
      "mean reward (100 episodes) 15.671111\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 222.560000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 253000\n",
      "learning_rate 0.000798\n",
      "mean reward (100 episodes) 12.012105\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 220.110000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 254000\n",
      "learning_rate 0.000797\n",
      "mean reward (100 episodes) 9.359211\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 204.680000\n",
      "max step (100 episodes) 954.000000\n",
      "------------------------------------------------\n",
      "timestep 255000\n",
      "learning_rate 0.000797\n",
      "mean reward (100 episodes) 5.025509\n",
      "max reward (100 episodes) 311.257263\n",
      "mean step (100 episodes) 195.170000\n",
      "max step (100 episodes) 954.000000\n",
      "------------------------------------------------\n",
      "timestep 256000\n",
      "learning_rate 0.000796\n",
      "mean reward (100 episodes) 0.371833\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 184.720000\n",
      "max step (100 episodes) 954.000000\n",
      "------------------------------------------------\n",
      "timestep 257000\n",
      "learning_rate 0.000796\n",
      "mean reward (100 episodes) -0.937600\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 172.360000\n",
      "max step (100 episodes) 774.000000\n",
      "------------------------------------------------\n",
      "timestep 258000\n",
      "learning_rate 0.000795\n",
      "mean reward (100 episodes) -2.485151\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 170.450000\n",
      "max step (100 episodes) 774.000000\n",
      "------------------------------------------------\n",
      "timestep 259000\n",
      "learning_rate 0.000794\n",
      "mean reward (100 episodes) -4.475017\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 168.860000\n",
      "max step (100 episodes) 774.000000\n",
      "saving model weights at /data1/Prophet/sluo/projects/SALE/result/dqn/agent/dqn_260000.ckpt\n",
      "------------------------------------------------\n",
      "timestep 260000\n",
      "learning_rate 0.000794\n",
      "mean reward (100 episodes) -4.380167\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 163.370000\n",
      "max step (100 episodes) 578.000000\n",
      "------------------------------------------------\n",
      "timestep 261000\n",
      "learning_rate 0.000793\n",
      "mean reward (100 episodes) -3.617241\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 156.130000\n",
      "max step (100 episodes) 578.000000\n",
      "------------------------------------------------\n",
      "timestep 262000\n",
      "learning_rate 0.000792\n",
      "mean reward (100 episodes) -7.462837\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 147.420000\n",
      "max step (100 episodes) 558.000000\n",
      "------------------------------------------------\n",
      "timestep 263000\n",
      "learning_rate 0.000792\n",
      "mean reward (100 episodes) -13.444500\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 142.970000\n",
      "max step (100 episodes) 558.000000\n",
      "------------------------------------------------\n",
      "timestep 264000\n",
      "learning_rate 0.000791\n",
      "mean reward (100 episodes) -14.911192\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 134.050000\n",
      "max step (100 episodes) 382.000000\n",
      "------------------------------------------------\n",
      "timestep 265000\n",
      "learning_rate 0.000791\n",
      "mean reward (100 episodes) -16.275419\n",
      "max reward (100 episodes) 296.332436\n",
      "mean step (100 episodes) 130.340000\n",
      "max step (100 episodes) 382.000000\n",
      "------------------------------------------------\n",
      "timestep 266000\n",
      "learning_rate 0.000790\n",
      "mean reward (100 episodes) -24.945653\n",
      "max reward (100 episodes) 265.632170\n",
      "mean step (100 episodes) 132.780000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 267000\n",
      "learning_rate 0.000789\n",
      "mean reward (100 episodes) -24.625534\n",
      "max reward (100 episodes) 265.632170\n",
      "mean step (100 episodes) 130.430000\n",
      "max step (100 episodes) 1000.000000\n",
      "------------------------------------------------\n",
      "timestep 268000\n",
      "learning_rate 0.000789\n",
      "mean reward (100 episodes) -28.571926\n",
      "max reward (100 episodes) 265.632170\n",
      "mean step (100 episodes) 122.110000\n",
      "max step (100 episodes) 1000.000000\n"
     ]
    }
   ],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = pd.Series(agent.eval_episode_rewards)\n",
    "steps = pd.Series(agent.eval_episode_steps)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 8))\n",
    "\n",
    "axes[0][0].plot(rewards.rolling(100, min_periods=20).mean())\n",
    "axes[0][0].set_title('mean reward')\n",
    "axes[0][1].plot(rewards.rolling(100, min_periods=20).max())\n",
    "axes[0][1].set_title('max reward')\n",
    "axes[1][0].plot(steps.rolling(100, min_periods=20).mean())\n",
    "axes[1][0].set_title('mean step')\n",
    "axes[1][1].plot(steps.rolling(100, min_periods=20).max())\n",
    "axes[1][1].set_title('max step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent_1, ..., agent_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data1/Prophet/sluo/projects/SALE/result/dqn/agent_0',\n",
       " '/data1/Prophet/sluo/projects/SALE/result/dqn/agent_1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf.agent_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "config['online'] = False\n",
    "config['max_training_steps'] = 200000\n",
    "config['lr'] = 5e-4\n",
    "config['decay_steps'] = 100000\n",
    "\n",
    "for idx in range(kf.n_splits):\n",
    "    config_idx = copy.deepcopy(config)\n",
    "    config_idx['persistent_directory'] = kf.train_paths[idx]\n",
    "    config_idx['checkpoint_path'] = kf.agent_paths[idx]\n",
    "    \n",
    "    agent_idx = QuantileAgent(name='LunarLander-v2', num_actions=4, config=config_idx)\n",
    "    agent_idx.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## behavior cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcs = []\n",
    "\n",
    "for k in range(kf.n_splits):\n",
    "    bc = BehaviorCloning(num_actions=agents[k].num_actions, verbose=1)\n",
    "    bc.train(agents[k].replay_buffer)\n",
    "    bcs.append(bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q values updator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, qvalues = kf.update_q(agents, bcs, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advantage learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_learner = AdvantageLearner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
