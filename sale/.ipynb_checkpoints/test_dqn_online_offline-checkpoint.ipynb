{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from utils.epsilon_decay import linearly_decaying_epsilon\n",
    "from models.box2d_models import DQNNetwork\n",
    "from replay_buffers.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "from dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(name='LunarLander-v2',\n",
    "                 # models\n",
    "                 network=DQNNetwork,\n",
    "                 num_actions=4,\n",
    "                 hiddens=[64,64],\n",
    "                 activation='relu',\n",
    "                 dueling=False,\n",
    "                 double=True,\n",
    "                 gamma=0.99,\n",
    "                 # optimizers\n",
    "                 optimizer=tf.keras.optimizers.Adam(\n",
    "                     tf.keras.optimizers.schedules.InverseTimeDecay(5e-4, decay_steps=100000, decay_rate=1)\n",
    "                 ),\n",
    "                 # replay buffer\n",
    "                 buffer_size=100000,\n",
    "                 min_replay_history=1000,               \n",
    "                 prioritized_replay=True,\n",
    "                 prioritized_replay_alpha=0.6,\n",
    "                 prioritized_replay_beta=0.4,\n",
    "                 online=True,\n",
    "                 persistent_directory='./trajs/dqn/test/',\n",
    "                 episode_counts_to_save=100,\n",
    "                 sample_steps_to_refresh=500,\n",
    "                 # training params\n",
    "                 max_training_steps=1000000,\n",
    "                 training_steps_to_eval=1000,\n",
    "                 batch_size=64,\n",
    "                 max_episode_steps=1000,\n",
    "                 # target model update params\n",
    "                 tau=0.999,\n",
    "                 update_period=1,\n",
    "                 target_update_period=1,\n",
    "                 # exploration params\n",
    "                 epsilon_fn=linearly_decaying_epsilon,                 \n",
    "                 epsilon_start=0.1,\n",
    "                 epsilon_decay_period=100000,\n",
    "                 epsilon_end=0.1,\n",
    "                 eval_mode=False,\n",
    "                 epsilon_eval=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = pd.Series(agent.eval_episode_rewards)\n",
    "steps = pd.Series(agent.eval_episode_steps)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 8))\n",
    "\n",
    "axes[0][0].plot(rewards.rolling(100, min_periods=20).mean())\n",
    "axes[0][0].set_title('mean reward')\n",
    "axes[0][1].plot(rewards.rolling(100, min_periods=20).max())\n",
    "axes[0][1].set_title('max reward')\n",
    "axes[1][0].plot(steps.rolling(100, min_periods=20).mean())\n",
    "axes[1][0].set_title('mean step')\n",
    "axes[1][1].plot(steps.rolling(100, min_periods=20).max())\n",
    "axes[1][1].set_title('max step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._eval(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('./saved/ddqn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(name='LunarLander-v2',\n",
    "                 # models\n",
    "                 network=DQNNetwork,\n",
    "                 num_actions=4,\n",
    "                 hiddens=[64,64],\n",
    "                 activation='relu',\n",
    "                 dueling=False,\n",
    "                 double=True,\n",
    "                 gamma=0.99,\n",
    "                 # optimizers\n",
    "                 optimizer=tf.keras.optimizers.Adam(\n",
    "                     tf.keras.optimizers.schedules.InverseTimeDecay(5e-4, decay_steps=100000, decay_rate=1)\n",
    "                 ),\n",
    "                 # replay buffer\n",
    "                 buffer_size=100000,\n",
    "                 min_replay_history=1000,               \n",
    "                 prioritized_replay=True,\n",
    "                 prioritized_replay_alpha=0.6,\n",
    "                 prioritized_replay_beta=0.4,\n",
    "                 online=True,\n",
    "                 persistent_directory='./trajs/dqn/test/',\n",
    "                 episode_counts_to_save=100,\n",
    "                 sample_steps_to_refresh=500,\n",
    "                 # training params\n",
    "                 max_training_steps=1000000,\n",
    "                 training_steps_to_eval=1000,\n",
    "                 batch_size=64,\n",
    "                 max_episode_steps=1000,\n",
    "                 # target model update params\n",
    "                 tau=0.999,\n",
    "                 update_period=1,\n",
    "                 target_update_period=1,\n",
    "                 # exploration params\n",
    "                 epsilon_fn=linearly_decaying_epsilon,                 \n",
    "                 epsilon_start=0.1,\n",
    "                 epsilon_decay_period=100000,\n",
    "                 epsilon_end=0.1,\n",
    "                 eval_mode=False,\n",
    "                 epsilon_eval=0.001)\n",
    "\n",
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = pd.Series(agent.eval_episode_rewards)\n",
    "steps = pd.Series(agent.eval_episode_steps)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 8))\n",
    "\n",
    "axes[0][0].plot(rewards.rolling(100, min_periods=20).mean())\n",
    "axes[0][0].set_title('mean reward')\n",
    "axes[0][1].plot(rewards.rolling(100, min_periods=20).max())\n",
    "axes[0][1].set_title('max reward')\n",
    "axes[1][0].plot(steps.rolling(100, min_periods=20).mean())\n",
    "axes[1][0].set_title('mean step')\n",
    "axes[1][1].plot(steps.rolling(100, min_periods=20).max())\n",
    "axes[1][1].set_title('max step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent_directory = './trajs/dqn/online/'\n",
    "# files = os.listdir(persistent_directory)\n",
    "# files = sorted([file for file in files if file.endswith('.pkl')])\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# trajs = []\n",
    "# for file in files:\n",
    "#     path = persistent_directory + file\n",
    "#     with open(path, 'rb') as f:\n",
    "#         trajs.append(pickle.load(f))\n",
    "        \n",
    "# trajs = [traj for file in trajs for traj in file] \n",
    "# # random.shuffle(trajs)\n",
    "\n",
    "# with open('./trajs/dqn/offline/trajs_dqn.pkl', 'wb') as f:\n",
    "#     pickle.dump(trajs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.keras.optimizers.schedules.InverseTimeDecay(5e-4, decay_steps=100000, decay_rate=1)\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "agent = DQNAgent(name='LunarLander-v2',\n",
    "                 num_actions=4,\n",
    "                 hiddens=[64,64],\n",
    "                 activation='relu',\n",
    "                 dueling=False,\n",
    "                 double=True,\n",
    "                 gamma=0.99,\n",
    "                 # optimizers\n",
    "                 optimizer=optimizer,\n",
    "                 # replay buffer\n",
    "                 buffer_size=10000,\n",
    "                 min_replay_history=1000,           \n",
    "                 prioritized_replay=False,\n",
    "                 prioritized_replay_alpha=0.6,\n",
    "                 prioritized_replay_beta=0.4,\n",
    "                 online=False,\n",
    "                 persistent_directory='./trajs/dqn/offline/',\n",
    "                 episode_counts_to_save=100,\n",
    "                 sample_steps_to_refresh=1000000,\n",
    "                 # training params\n",
    "                 max_training_steps=500000,\n",
    "                 training_steps_to_eval=1000,\n",
    "                 batch_size=64,\n",
    "                 max_episode_steps=1000,\n",
    "                 # target model update params\n",
    "                 tau=0.999,\n",
    "                 update_period=1,\n",
    "                 target_update_period=1,\n",
    "                 # exploration params\n",
    "                 epsilon_fn=linearly_decaying_epsilon,             \n",
    "                 epsilon_start=0.1,\n",
    "                 epsilon_decay_period=100000,\n",
    "                 epsilon_end=0.1,\n",
    "                 eval_mode=False,\n",
    "                 epsilon_eval=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = pd.Series(agent.eval_episode_rewards)\n",
    "steps = pd.Series(agent.eval_episode_steps)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 8))\n",
    "\n",
    "axes[0][0].plot(rewards.rolling(100, min_periods=20).mean())\n",
    "axes[0][0].set_title('mean reward')\n",
    "axes[0][1].plot(rewards.rolling(100, min_periods=20).max())\n",
    "axes[0][1].set_title('max reward')\n",
    "axes[1][0].plot(steps.rolling(100, min_periods=20).mean())\n",
    "axes[1][0].set_title('mean step')\n",
    "axes[1][1].plot(steps.rolling(100, min_periods=20).max())\n",
    "axes[1][1].set_title('max step')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
